{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier way of using the context of the previously happening single character to predict the next one was good but we weren't producing very name like sounding things.\n",
    "\n",
    "Problem with this approach is\n",
    "The table we saw in the previous file (tells deals with predicting the next character in a sequence in context to the previous one) quickly blows up and its size grows exponentially with the length of context.\n",
    "\n",
    "This is because if we take a single character at a time, we will have 27 different possibilities of context that is 27 different rows. But if we were to consider the previous context of 2 characters to predict the next then we will have 27*27= 729 rows/possibilities of context. and it goes on\n",
    "\n",
    "We will have too many rows and way too many counts for each possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the research paper [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf),\n",
    "\n",
    "it deals with the word level language modelling and it does have a vocabulary of 17000 possible words and each word is assosciated as a 30 dimensional feature vector. Literaly 17000 points or vectors in a 30 dimensional vector space.\n",
    "\n",
    "Initially these words are initialised randomly, spreaded out randomly. Then we tune the embeddings of these words using **backpropogation**. \n",
    "\n",
    "So during the course of training of these neural networks go around in the space and words with almost similar meanings or are synonyms of each other end up in a similar part of space and conversely words with different meanings go away from each other.\n",
    "\n",
    "\n",
    "Similar to our previous work, in the research paper they,\n",
    "- try to predict the next word given the previous ones\n",
    "- to train the neural network, they maximize the log likelihood of the training data\n",
    "So the modelling approach is all identical\n",
    "\n",
    "In the paper a example is sited,\n",
    "\n",
    "The phrase **A dog was running in a room**\n",
    "is likewise to **The cat is running in a room**\n",
    "\n",
    "Mayb the network may have realised that a and the are frequently interchangeable and therefore the network may have put the embeddings of 'a' and 'the' near in the space and therefore the similarity is drawn. Therefore we can transfer knowledge through that embedding and we can generalise in that way. In the same way they may have found similarity in the 'dog' and 'cat' too therefore a mapping happened there too causing a knowledge transfer and a generalisation to novel scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/neural_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we take 3 previous words to predict the upcoming 4th word in a sequence. These 3 are the index of the incoming word and because there are 17k words these indices are integers between 0 and 16999. \n",
    "\n",
    "Along with this we have a lookup table C which is a matrix 17k x 30.\n",
    "\n",
    "This is considered as a lookup table and every index is actually plucking a row out of this embedding matrix so that each index is converted to a 30 dimensional vector that corresponds to the embedding vector for that word.\n",
    "\n",
    "We have the input layer of 30 neurons for 3 words making up 90 neurons and C is being shared across all the words so we are indexing to the same matrix again and again for each one of these words.\n",
    "\n",
    "Size of the hidden layer of this neural network is a _hyper parameter_ and this can be as large or small it can be. We will be working out multiple choices of sizes/different lengths of these hidden layers and then evaluate how well they do.\n",
    "\n",
    "Lets say we have 100 neurons out of which all of them will be fully connected to the 90 words or 90 numbers that make up these three words so this is a fully connected layer and then there is a tan h long linearity.\n",
    "\n",
    "In case of the output layer, there are 17k possible words that could appear next and all of these are fully connected to tall the neurons in the hidden layer.  Since there are a lot of words, there are lot of parameters here and therefore a lot of computation here.\n",
    "\n",
    "Therefore we can also say there are ```17k logits``` here and at the top we have the softmax layer which exponentiates these logits, normalised to sum of 1 to get the best probability distribution for next word in sequence. In training we do have the label/ identity of the next word in the sequence, that index is used to pluck out the probability of that word and they are maximised  the probability of that word with respect to the parameters of the neural network. \n",
    "\n",
    "These parameters are nothing but the ```weights``` and ```biases``` of the output layer and of the hidden kayer and the embedding lookup table of C (matrix). All of these are optimised using the back propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all words\n",
    "words = open('../../names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "characters = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(characters)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... --------> e\n",
      "..e --------> m\n",
      ".em --------> m\n",
      "emm --------> a\n",
      "mma --------> .\n",
      "olivia\n",
      "... --------> o\n",
      "..o --------> l\n",
      ".ol --------> i\n",
      "oli --------> v\n",
      "liv --------> i\n",
      "ivi --------> a\n",
      "via --------> .\n",
      "ava\n",
      "... --------> a\n",
      "..a --------> v\n",
      ".av --------> a\n",
      "ava --------> .\n",
      "isabella\n",
      "... --------> i\n",
      "..i --------> s\n",
      ".is --------> a\n",
      "isa --------> b\n",
      "sab --------> e\n",
      "abe --------> l\n",
      "bel --------> l\n",
      "ell --------> a\n",
      "lla --------> .\n",
      "sophia\n",
      "... --------> s\n",
      "..s --------> o\n",
      ".so --------> p\n",
      "sop --------> h\n",
      "oph --------> i\n",
      "phi --------> a\n",
      "hia --------> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length we use to predict the next character\n",
    "X,Y = [],[] # X= input to the neural net, Y=label\n",
    "for word in words[:5]:\n",
    "    print(word)\n",
    "    context = [0] * block_size # padded context of 0 tokens\n",
    "    \n",
    "    for character in word + '.':\n",
    "        index = stoi[character] \n",
    "        X.append(context) # current running context\n",
    "        Y.append(index)\n",
    "        print(''.join(itos[i] for i in context), '-------->', itos[index])\n",
    "        context =  context[1:] + [index] # crop the context and append the new character\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a neural network It will take the input X and predict Y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Embedding lookup table C \n",
    "    We have 27 different characters and we will be embedding them in a lower level space. In the paper, they have a total of 17000 words and embed them in small dimensions as 30 (Cramping up 17k words in 30 dimensional space). \n",
    "    \n",
    "    In our case similarly lets cramp them up to a 2 dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3847,  0.7616], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Embedding a single integer to X C[5] \n",
    "C[5]# by taking the random number generated in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes = 27) #2nd way of doing this. \n",
    "# embedding is done here in such a way that we get all 0's and just at 5 we have the value turned on as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: long int != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: long int != float"
     ]
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes = 27) @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.dtype, F.one_hot(torch.tensor(5), num_classes = 27).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multiplication between a 64 bit integer and a float won't happen.\n",
    "@ can't do that\n",
    "There we need to convert the one hot encoded one into a float first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0916, 0.2925])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes = 27).float() @ C\n",
    "# we end up masking in all the rows and finally put out that 5th row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at it carefully we will find out C[5] is generated here. This is because we stripped out the 5 th row masking out the rest.\n",
    "\n",
    "So one way to look at it is,\n",
    "- an integer indexing into a lookup table C.\n",
    "OR\n",
    "- first layer to the bigger neural net. A layer which have linear neurons that have no nonlinearlity with the tan h and its weight matrix is C\n",
    "And we are encoding those integers using one hot and feeding them into the neural net\n",
    "\n",
    "\n",
    "So embedding a single integer is easy because we can easily retrieve them.\n",
    "What to do if we have to embed all the 32 integers in X simultaneously?\n",
    "\n",
    "We can index C with a single integer, a list of integers that are indices, a tensor of the integers or even a multi dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.0916,  0.2925]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.0916,  0.2925],\n",
       "          [-0.6577, -1.7706]],\n",
       " \n",
       "         [[ 1.0916,  0.2925],\n",
       "          [-0.6577, -1.7706],\n",
       "          [-0.6577, -1.7706]],\n",
       " \n",
       "         [[-0.6577, -1.7706],\n",
       "          [-0.6577, -1.7706],\n",
       "          [-0.1823, -0.1603]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.0128,  0.3325]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.0128,  0.3325],\n",
       "          [-1.5208,  0.8414]],\n",
       " \n",
       "         [[ 1.0128,  0.3325],\n",
       "          [-1.5208,  0.8414],\n",
       "          [-0.3424,  0.1001]],\n",
       " \n",
       "         [[-1.5208,  0.8414],\n",
       "          [-0.3424,  0.1001],\n",
       "          [-1.1273,  0.5526]],\n",
       " \n",
       "         [[-0.3424,  0.1001],\n",
       "          [-1.1273,  0.5526],\n",
       "          [-0.3424,  0.1001]],\n",
       " \n",
       "         [[-1.1273,  0.5526],\n",
       "          [-0.3424,  0.1001],\n",
       "          [-0.1823, -0.1603]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [-0.1823, -0.1603]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [-0.1823, -0.1603],\n",
       "          [-1.1273,  0.5526]],\n",
       " \n",
       "         [[-0.1823, -0.1603],\n",
       "          [-1.1273,  0.5526],\n",
       "          [-0.1823, -0.1603]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [-0.3424,  0.1001]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [-0.3424,  0.1001],\n",
       "          [-0.5990,  0.5137]],\n",
       " \n",
       "         [[-0.3424,  0.1001],\n",
       "          [-0.5990,  0.5137],\n",
       "          [-0.1823, -0.1603]],\n",
       " \n",
       "         [[-0.5990,  0.5137],\n",
       "          [-0.1823, -0.1603],\n",
       "          [-1.3208,  0.4055]],\n",
       " \n",
       "         [[-0.1823, -0.1603],\n",
       "          [-1.3208,  0.4055],\n",
       "          [ 1.0916,  0.2925]],\n",
       " \n",
       "         [[-1.3208,  0.4055],\n",
       "          [ 1.0916,  0.2925],\n",
       "          [-1.5208,  0.8414]],\n",
       " \n",
       "         [[ 1.0916,  0.2925],\n",
       "          [-1.5208,  0.8414],\n",
       "          [-1.5208,  0.8414]],\n",
       " \n",
       "         [[-1.5208,  0.8414],\n",
       "          [-1.5208,  0.8414],\n",
       "          [-0.1823, -0.1603]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [ 1.1696,  1.4134],\n",
       "          [-0.5990,  0.5137]],\n",
       " \n",
       "         [[ 1.1696,  1.4134],\n",
       "          [-0.5990,  0.5137],\n",
       "          [ 1.0128,  0.3325]],\n",
       " \n",
       "         [[-0.5990,  0.5137],\n",
       "          [ 1.0128,  0.3325],\n",
       "          [-1.2333,  1.2925]],\n",
       " \n",
       "         [[ 1.0128,  0.3325],\n",
       "          [-1.2333,  1.2925],\n",
       "          [-0.0336,  1.8229]],\n",
       " \n",
       "         [[-1.2333,  1.2925],\n",
       "          [-0.0336,  1.8229],\n",
       "          [-0.3424,  0.1001]],\n",
       " \n",
       "         [[-0.0336,  1.8229],\n",
       "          [-0.3424,  0.1001],\n",
       "          [-0.1823, -0.1603]]]),\n",
       " torch.Size([32, 3, 2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X], C[X].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we understand we have retained the 32 x 3 original shape of X and along with that we have kept the retrieved the embedding vector for every 32x3 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1823, -0.1603]), tensor([-0.1823, -0.1603]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13,2], C[1] # Both are same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the help of pytorch's amazing way of embedding, we can embed all the values simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = C[X]\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Construction of the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs to this layer is 3 times 2 because we have  3 units of 2 dimensional embeddings. \n",
    "- The number of neurons in the tan h layer is variable and lets assume it to be 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal way to construct this is to do <br>\n",
    "```python\n",
    "embedding @ W1 +b1\n",
    "```\n",
    "\n",
    "But this is not possible since embedding is 32 x 3 x 2. This is because these embeddings are stacked up in the dimensions of input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39mb1\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "embedding @ W1 +b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.0916,  0.2925],\n",
       "        [ 1.1696,  1.4134,  1.0916,  0.2925, -0.6577, -1.7706],\n",
       "        [ 1.0916,  0.2925, -0.6577, -1.7706, -0.6577, -1.7706],\n",
       "        [-0.6577, -1.7706, -0.6577, -1.7706, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.0128,  0.3325],\n",
       "        [ 1.1696,  1.4134,  1.0128,  0.3325, -1.5208,  0.8414],\n",
       "        [ 1.0128,  0.3325, -1.5208,  0.8414, -0.3424,  0.1001],\n",
       "        [-1.5208,  0.8414, -0.3424,  0.1001, -1.1273,  0.5526],\n",
       "        [-0.3424,  0.1001, -1.1273,  0.5526, -0.3424,  0.1001],\n",
       "        [-1.1273,  0.5526, -0.3424,  0.1001, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134, -0.1823, -0.1603, -1.1273,  0.5526],\n",
       "        [-0.1823, -0.1603, -1.1273,  0.5526, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134, -0.3424,  0.1001],\n",
       "        [ 1.1696,  1.4134, -0.3424,  0.1001, -0.5990,  0.5137],\n",
       "        [-0.3424,  0.1001, -0.5990,  0.5137, -0.1823, -0.1603],\n",
       "        [-0.5990,  0.5137, -0.1823, -0.1603, -1.3208,  0.4055],\n",
       "        [-0.1823, -0.1603, -1.3208,  0.4055,  1.0916,  0.2925],\n",
       "        [-1.3208,  0.4055,  1.0916,  0.2925, -1.5208,  0.8414],\n",
       "        [ 1.0916,  0.2925, -1.5208,  0.8414, -1.5208,  0.8414],\n",
       "        [-1.5208,  0.8414, -1.5208,  0.8414, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134, -0.5990,  0.5137],\n",
       "        [ 1.1696,  1.4134, -0.5990,  0.5137,  1.0128,  0.3325],\n",
       "        [-0.5990,  0.5137,  1.0128,  0.3325, -1.2333,  1.2925],\n",
       "        [ 1.0128,  0.3325, -1.2333,  1.2925, -0.0336,  1.8229],\n",
       "        [-1.2333,  1.2925, -0.0336,  1.8229, -0.3424,  0.1001],\n",
       "        [-0.0336,  1.8229, -0.3424,  0.1001, -0.1823, -0.1603]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To solve this above error, we need to concatinate the all the 3 units (C)\n",
    "# 32 X 3 X 2 TO 32 X 6\n",
    "# The cat function in Torch gives the sequence of tensors in the given dimension\n",
    "\n",
    "embedding[:, 0, :] # 32 x 2 embeddings of the 1st word (C  unit)\n",
    "embedding[:, 1, :] # 32 x 2 embeddings of the 2nd word (C  unit)\n",
    "embedding[:, 2, :] # 32 x 2 embeddings of the 3rd word (C  unit)\n",
    "\n",
    "torch.cat([embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]],1).shape\n",
    "# took 32 nd squashed the the other 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are indexing directly, but if the block size increases we need to change the code completely since I have indexed here directly. This is not what we need so we need to handle that too.\n",
    "\n",
    "For this, we can make use of ```torch.unbind``` that removes a tensor dimension and returns a tuple of all sizes along a given dimension already without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.0916,  0.2925],\n",
       "         [-0.6577, -1.7706],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.0128,  0.3325],\n",
       "         [-1.5208,  0.8414],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-1.1273,  0.5526],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.1823, -0.1603],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-0.5990,  0.5137],\n",
       "         [-0.1823, -0.1603],\n",
       "         [-1.3208,  0.4055],\n",
       "         [ 1.0916,  0.2925],\n",
       "         [-1.5208,  0.8414],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.5990,  0.5137],\n",
       "         [ 1.0128,  0.3325],\n",
       "         [-1.2333,  1.2925],\n",
       "         [-0.0336,  1.8229]]),\n",
       " tensor([[ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.0916,  0.2925],\n",
       "         [-0.6577, -1.7706],\n",
       "         [-0.6577, -1.7706],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.0128,  0.3325],\n",
       "         [-1.5208,  0.8414],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-1.1273,  0.5526],\n",
       "         [-0.3424,  0.1001],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.1823, -0.1603],\n",
       "         [-1.1273,  0.5526],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-0.5990,  0.5137],\n",
       "         [-0.1823, -0.1603],\n",
       "         [-1.3208,  0.4055],\n",
       "         [ 1.0916,  0.2925],\n",
       "         [-1.5208,  0.8414],\n",
       "         [-1.5208,  0.8414],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.5990,  0.5137],\n",
       "         [ 1.0128,  0.3325],\n",
       "         [-1.2333,  1.2925],\n",
       "         [-0.0336,  1.8229],\n",
       "         [-0.3424,  0.1001]]),\n",
       " tensor([[ 1.1696,  1.4134],\n",
       "         [ 1.0916,  0.2925],\n",
       "         [-0.6577, -1.7706],\n",
       "         [-0.6577, -1.7706],\n",
       "         [-0.1823, -0.1603],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [ 1.0128,  0.3325],\n",
       "         [-1.5208,  0.8414],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-1.1273,  0.5526],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-0.1823, -0.1603],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.1823, -0.1603],\n",
       "         [-1.1273,  0.5526],\n",
       "         [-0.1823, -0.1603],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-0.5990,  0.5137],\n",
       "         [-0.1823, -0.1603],\n",
       "         [-1.3208,  0.4055],\n",
       "         [ 1.0916,  0.2925],\n",
       "         [-1.5208,  0.8414],\n",
       "         [-1.5208,  0.8414],\n",
       "         [-0.1823, -0.1603],\n",
       "         [ 1.1696,  1.4134],\n",
       "         [-0.5990,  0.5137],\n",
       "         [ 1.0128,  0.3325],\n",
       "         [-1.2333,  1.2925],\n",
       "         [-0.0336,  1.8229],\n",
       "         [-0.3424,  0.1001],\n",
       "         [-0.1823, -0.1603]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(embedding, 1) \n",
    "# this becomes equivalent to [embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(embedding,1),1).shape # it doesn't matter what the size of the block is\n",
    "\n",
    "# But this is a highly inefficient way to do this as it will create a lot of memory because its not manipulating the view but creating a whole new set of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# better way to do this\n",
    "a = torch.arange(18)\n",
    "a.view(2,9) # we manipulate the shapes of the tensor \n",
    "# The only condition is \n",
    "a.view(3,3,2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating a tensor we will have a single storage with a one dimensional sequence of 18 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1445/214256462.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But .view() is a very efficient way of manipulating this storage. It manipulates this one dimensional into a n dimensional tensor. No memory is changed, copied or moved. The storage is identical. Some of the attributes related to view of the tensor are manipulated. The attributes like the storage offset, strides and shapes and they are manipulated so that this one dimensional space is seen as different n dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.0916,  0.2925],\n",
       "        [ 1.1696,  1.4134,  1.0916,  0.2925, -0.6577, -1.7706],\n",
       "        [ 1.0916,  0.2925, -0.6577, -1.7706, -0.6577, -1.7706],\n",
       "        [-0.6577, -1.7706, -0.6577, -1.7706, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.0128,  0.3325],\n",
       "        [ 1.1696,  1.4134,  1.0128,  0.3325, -1.5208,  0.8414],\n",
       "        [ 1.0128,  0.3325, -1.5208,  0.8414, -0.3424,  0.1001],\n",
       "        [-1.5208,  0.8414, -0.3424,  0.1001, -1.1273,  0.5526],\n",
       "        [-0.3424,  0.1001, -1.1273,  0.5526, -0.3424,  0.1001],\n",
       "        [-1.1273,  0.5526, -0.3424,  0.1001, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134, -0.1823, -0.1603, -1.1273,  0.5526],\n",
       "        [-0.1823, -0.1603, -1.1273,  0.5526, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134, -0.3424,  0.1001],\n",
       "        [ 1.1696,  1.4134, -0.3424,  0.1001, -0.5990,  0.5137],\n",
       "        [-0.3424,  0.1001, -0.5990,  0.5137, -0.1823, -0.1603],\n",
       "        [-0.5990,  0.5137, -0.1823, -0.1603, -1.3208,  0.4055],\n",
       "        [-0.1823, -0.1603, -1.3208,  0.4055,  1.0916,  0.2925],\n",
       "        [-1.3208,  0.4055,  1.0916,  0.2925, -1.5208,  0.8414],\n",
       "        [ 1.0916,  0.2925, -1.5208,  0.8414, -1.5208,  0.8414],\n",
       "        [-1.5208,  0.8414, -1.5208,  0.8414, -0.1823, -0.1603],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134,  1.1696,  1.4134],\n",
       "        [ 1.1696,  1.4134,  1.1696,  1.4134, -0.5990,  0.5137],\n",
       "        [ 1.1696,  1.4134, -0.5990,  0.5137,  1.0128,  0.3325],\n",
       "        [-0.5990,  0.5137,  1.0128,  0.3325, -1.2333,  1.2925],\n",
       "        [ 1.0128,  0.3325, -1.2333,  1.2925, -0.0336,  1.8229],\n",
       "        [-1.2333,  1.2925, -0.0336,  1.8229, -0.3424,  0.1001],\n",
       "        [-0.0336,  1.8229, -0.3424,  0.1001, -0.1823, -0.1603]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.view(32,6) # the 3 and 2 gets stacked up in a single row. similar to the concat and provides the exact similar output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2901,  0.0411,  0.3131,  ...,  0.3433, -2.8804, -0.3924],\n",
       "        [-2.5487,  0.3683,  0.5410,  ...,  0.0306, -3.0687, -0.5475],\n",
       "        [-1.4021,  2.0430, -3.5930,  ..., -0.6332, -4.6154, -2.9854],\n",
       "        ...,\n",
       "        [ 0.3353, -3.6396, -2.9627,  ...,  1.4435,  0.7158,  1.1763],\n",
       "        [ 0.0293, -2.9802,  2.6207,  ...,  2.2869, -0.4369,  2.1615],\n",
       "        [-0.8324, -2.6727, -0.9177,  ...,  0.2334, -5.3951, -0.5054]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally we can construct the hidden layer\n",
    "hidden_layer = embedding.view(32,6) @ W1 + b1\n",
    "hidden_layer # all the hidden states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer.shape # the 100 dimension activations for every one of the 32 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2901,  0.0411,  0.3131,  ...,  0.3433, -2.8804, -0.3924],\n",
       "        [-2.5487,  0.3683,  0.5410,  ...,  0.0306, -3.0687, -0.5475],\n",
       "        [-1.4021,  2.0430, -3.5930,  ..., -0.6332, -4.6154, -2.9854],\n",
       "        ...,\n",
       "        [ 0.3353, -3.6396, -2.9627,  ...,  1.4435,  0.7158,  1.1763],\n",
       "        [ 0.0293, -2.9802,  2.6207,  ...,  2.2869, -0.4369,  2.1615],\n",
       "        [-0.8324, -2.6727, -0.9177,  ...,  0.2334, -5.3951, -0.5054]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for generalising\n",
    "\n",
    "hidden_layer = embedding.view(embedding.shape[0], 6) @ W1 + b1\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2901,  0.0411,  0.3131,  ...,  0.3433, -2.8804, -0.3924],\n",
       "        [-2.5487,  0.3683,  0.5410,  ...,  0.0306, -3.0687, -0.5475],\n",
       "        [-1.4021,  2.0430, -3.5930,  ..., -0.6332, -4.6154, -2.9854],\n",
       "        ...,\n",
       "        [ 0.3353, -3.6396, -2.9627,  ...,  1.4435,  0.7158,  1.1763],\n",
       "        [ 0.0293, -2.9802,  2.6207,  ...,  2.2869, -0.4369,  2.1615],\n",
       "        [-0.8324, -2.6727, -0.9177,  ...,  0.2334, -5.3951, -0.5054]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR\n",
    "\n",
    "hidden_layer = embedding.view(-1, 6) @ W1 + b1\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8592,  0.0411,  0.3033,  ...,  0.3304, -0.9937, -0.3734],\n",
       "        [-0.9878,  0.3525,  0.4937,  ...,  0.0306, -0.9957, -0.4987],\n",
       "        [-0.8858,  0.9669, -0.9985,  ..., -0.5602, -0.9998, -0.9949],\n",
       "        ...,\n",
       "        [ 0.3233, -0.9986, -0.9947,  ...,  0.8944,  0.6143,  0.8263],\n",
       "        [ 0.0292, -0.9949,  0.9895,  ...,  0.9796, -0.4111,  0.9738],\n",
       "        [-0.6818, -0.9905, -0.7248,  ...,  0.2292, -1.0000, -0.4663]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer = torch.tanh(embedding.view(-1, 6) @ W1 + b1)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the values are between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]), torch.Size([100]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer = (embedding.view(-1, 6) @ W1)\n",
    "hidden_layer.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be having broadcasting done here\n",
    "# 32 X 100 is broadcasted to 100\n",
    "#      100\n",
    "# boradcast will align to the right and then enter in a fake dimension in the first place.\n",
    "# 32 X 100 \n",
    "#  1 X 100 \n",
    "# copies all the 32 rows, every one of this and then do a element wise addition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Construction of the final output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 100 inputs and since we have 27 characters there will be 27 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5307e+00, -6.1167e+00, -7.7523e+00,  1.1603e+01,  1.2361e+01,\n",
       "          2.1246e+01, -5.0853e+01, -2.4711e+00, -1.3251e+01, -1.5714e+01,\n",
       "          5.7926e+01, -2.3510e+01,  2.2106e+01, -2.1171e+01,  1.6980e+01,\n",
       "          2.9062e+01,  1.9111e+01,  2.9771e+01, -5.1988e+00,  1.3259e+01,\n",
       "         -2.8121e+01, -2.1592e+01,  1.2597e+01, -1.6221e+01, -1.6764e+01,\n",
       "         -1.5168e+01, -2.8787e+01],\n",
       "        [-4.4348e+00, -4.0918e+00, -7.1336e+00,  1.7271e+01,  2.4175e+01,\n",
       "          2.0607e+01, -2.4195e+01, -8.9282e+00, -2.3683e+00, -1.2514e+01,\n",
       "          5.5166e+01, -1.6548e+01,  1.2500e+01, -5.1127e+00,  1.5303e+01,\n",
       "          3.7853e+01,  4.3254e+00,  9.0689e+00, -3.5606e+00,  1.2535e+01,\n",
       "         -3.8439e+01, -3.6996e+01,  2.0127e+01, -8.8383e+00, -1.5734e+01,\n",
       "         -1.9581e+01, -2.5485e+01],\n",
       "        [ 2.7976e+00,  2.8904e+01, -1.4060e+01,  3.6962e+01,  4.3405e+01,\n",
       "          6.3184e+00,  4.1204e+01, -5.4497e+00,  1.2795e+01, -2.1973e+01,\n",
       "          1.1750e+01, -2.7849e+00, -2.0616e+01,  3.0502e+00,  1.1070e+01,\n",
       "          1.7847e+01, -8.4016e-01, -4.0074e+01, -4.3005e+00,  4.5838e+01,\n",
       "         -6.1744e+01, -7.7606e+01,  4.7663e+01,  2.7495e+01, -1.7411e+01,\n",
       "         -1.4156e+01, -1.3684e+01],\n",
       "        [ 1.6328e+01,  5.8780e+01,  1.7971e+01,  2.1668e+01,  1.4097e+01,\n",
       "         -2.5191e+01,  7.5235e+01,  5.0745e+00,  2.0717e+01, -2.2896e+01,\n",
       "         -4.3753e+01, -1.6959e+01, -3.2268e+01, -1.4186e+01,  2.3214e+00,\n",
       "          7.7176e+00, -1.5012e+01, -4.2792e+01, -1.2015e+00,  3.1160e+01,\n",
       "         -3.2625e+01, -6.1257e+01,  2.5813e+01,  5.6041e+00,  1.4216e-01,\n",
       "          1.4378e+01,  1.4692e+01],\n",
       "        [ 6.1556e+00,  1.7710e+01,  1.3595e+01, -1.6338e+01, -1.7370e+01,\n",
       "         -1.4996e+01,  1.5812e+01, -5.5099e+00, -5.2870e+00,  1.5603e+01,\n",
       "         -4.2718e+01,  4.4300e+00, -1.3182e+01, -5.0359e+00, -1.0166e+01,\n",
       "         -2.4816e+01,  3.0857e+00,  5.9484e+00, -1.1569e+00, -1.9463e+01,\n",
       "          3.1224e+01,  4.5783e+01, -2.6895e+01, -6.3780e-01,  1.9469e+01,\n",
       "          1.6559e+01,  4.3025e+01],\n",
       "        [ 3.5307e+00, -6.1167e+00, -7.7523e+00,  1.1603e+01,  1.2361e+01,\n",
       "          2.1246e+01, -5.0853e+01, -2.4711e+00, -1.3251e+01, -1.5714e+01,\n",
       "          5.7926e+01, -2.3510e+01,  2.2106e+01, -2.1171e+01,  1.6980e+01,\n",
       "          2.9062e+01,  1.9111e+01,  2.9771e+01, -5.1988e+00,  1.3259e+01,\n",
       "         -2.8121e+01, -2.1592e+01,  1.2597e+01, -1.6221e+01, -1.6764e+01,\n",
       "         -1.5168e+01, -2.8787e+01],\n",
       "        [-3.6079e+00, -3.7719e+00, -7.7366e+00,  1.7489e+01,  2.3510e+01,\n",
       "          2.0624e+01, -2.4913e+01, -7.9591e+00, -2.6945e+00, -1.2826e+01,\n",
       "          5.3491e+01, -1.6843e+01,  1.2344e+01, -5.9889e+00,  1.5425e+01,\n",
       "          3.6121e+01,  5.6305e+00,  9.3714e+00, -3.5496e+00,  1.4204e+01,\n",
       "         -3.8429e+01, -3.6847e+01,  2.0279e+01, -7.8890e+00, -1.5682e+01,\n",
       "         -1.8718e+01, -2.5867e+01],\n",
       "        [ 2.7870e+01,  2.8083e+01, -2.1899e+01,  2.8583e+01,  1.1469e+01,\n",
       "          7.3840e+00, -1.7341e+01,  1.9790e+01, -1.0688e+01, -3.1450e+01,\n",
       "         -6.0039e+00, -2.0310e+01, -4.3667e+00, -3.7514e+01,  1.5650e+01,\n",
       "         -1.9852e+01,  4.1716e+01,  2.1836e+00, -6.4813e+00,  6.7995e+01,\n",
       "         -4.1654e+01, -4.7148e+01,  3.4685e+01,  2.4841e+01, -1.8023e+01,\n",
       "          6.0927e+00, -2.5708e+01],\n",
       "        [ 1.7268e+00, -1.7335e+00,  2.4547e+01,  1.6653e+00, -2.5470e+01,\n",
       "         -4.8438e+00,  1.5286e+01,  2.2043e+01,  2.6765e+01,  2.6679e-04,\n",
       "         -4.5945e+01, -3.8806e+01, -8.6444e-01, -1.5018e+00,  7.1796e+00,\n",
       "          2.3520e+01, -2.8957e+01, -8.1734e+00,  1.5007e+01,  1.6105e+01,\n",
       "         -1.0325e+01, -3.0341e+01, -7.8662e+00, -1.9483e+01,  1.5323e+01,\n",
       "          3.1284e+01, -2.3799e+01],\n",
       "        [ 9.7468e+00, -2.7736e+01, -1.6836e+01, -9.7837e+00, -5.2724e+00,\n",
       "         -1.4693e+01, -9.3098e+00,  1.5410e+01, -1.7928e+01, -6.5556e+00,\n",
       "          1.6750e+00,  3.5591e+01,  6.4902e+00, -7.2024e-01, -1.2102e+01,\n",
       "         -2.5426e+01, -2.6735e+00, -9.0237e+00,  2.5924e+00,  3.0579e+00,\n",
       "          2.4503e+01, -4.2919e+00,  8.2379e+00,  1.9030e+01, -2.2466e+01,\n",
       "         -1.2848e+01, -2.7133e+01],\n",
       "        [-2.5599e+00, -2.1018e+01,  1.0428e+01, -9.8402e+00, -1.9022e+01,\n",
       "         -7.4913e+00,  5.5584e+00,  1.2715e+01,  9.3758e+00,  7.6306e+00,\n",
       "         -2.4429e+01, -2.4791e+00,  3.1318e+00,  9.1232e+00, -4.0788e+00,\n",
       "          6.3873e+00, -2.5306e+01, -6.8822e+00,  1.1175e+01, -5.1847e+00,\n",
       "          1.3570e+01, -4.0214e+00, -1.0917e+01, -6.8115e+00,  5.1980e+00,\n",
       "          1.1140e+01, -1.7356e+01],\n",
       "        [-1.7678e+00, -2.3906e+01, -5.5662e+00, -9.0110e+00,  2.2177e+00,\n",
       "         -1.1275e+01,  5.0149e+00,  2.0648e+00, -7.4799e+00,  3.0529e-01,\n",
       "          1.1481e+01,  2.8919e+01,  3.9313e+00,  1.0878e+01, -1.0436e+01,\n",
       "         -1.3918e+00, -1.8062e+01, -1.3155e+01,  3.1451e+00, -1.2985e+01,\n",
       "          1.7049e+01, -6.2624e+00,  4.0149e+00,  6.6261e+00, -1.5083e+01,\n",
       "         -1.5982e+01, -1.5792e+01],\n",
       "        [ 3.5307e+00, -6.1167e+00, -7.7523e+00,  1.1603e+01,  1.2361e+01,\n",
       "          2.1246e+01, -5.0853e+01, -2.4711e+00, -1.3251e+01, -1.5714e+01,\n",
       "          5.7926e+01, -2.3510e+01,  2.2106e+01, -2.1171e+01,  1.6980e+01,\n",
       "          2.9062e+01,  1.9111e+01,  2.9771e+01, -5.1988e+00,  1.3259e+01,\n",
       "         -2.8121e+01, -2.1592e+01,  1.2597e+01, -1.6221e+01, -1.6764e+01,\n",
       "         -1.5168e+01, -2.8787e+01],\n",
       "        [ 6.0234e-01,  2.6974e+00, -1.5725e+01,  2.5968e+01,  2.5248e+01,\n",
       "          2.0258e+01, -9.8478e+00, -2.9619e-01,  2.9805e+00, -1.4234e+01,\n",
       "          2.7057e+01, -1.4441e+01,  1.0211e+00, -3.2318e+00,  1.5567e+01,\n",
       "          1.9814e+01,  1.0179e+01, -5.9463e+00, -1.8408e+00,  3.7247e+01,\n",
       "         -4.8069e+01, -4.9329e+01,  2.9577e+01,  1.2608e+01, -1.3972e+01,\n",
       "         -1.0628e+01, -2.8162e+01],\n",
       "        [ 2.7581e+01,  3.1049e+01, -8.4395e-01,  2.0835e+01, -5.4924e-01,\n",
       "         -1.0014e+01,  7.3790e+00,  2.5703e+01,  1.2865e+00, -3.3216e+01,\n",
       "         -2.2097e+01, -2.6597e+01, -6.0114e+00, -3.6132e+01,  1.0962e+01,\n",
       "         -4.8163e+00,  1.4181e+01, -8.7278e+00, -6.5086e-01,  5.3711e+01,\n",
       "         -3.0757e+01, -5.7430e+01,  2.7909e+01,  5.5711e+00, -1.3890e+01,\n",
       "          1.4393e+01, -2.6802e+01],\n",
       "        [-6.3188e+00, -1.8590e+01,  1.3847e+01, -9.2663e+00, -1.8193e+01,\n",
       "         -4.9933e+00,  1.0522e+01,  8.7522e+00,  1.3896e+01,  1.2049e+01,\n",
       "         -2.7656e+01, -6.3887e+00,  6.0991e-01,  1.3459e+01, -3.1477e+00,\n",
       "          1.1098e+01, -2.8346e+01, -7.7964e+00,  1.1733e+01, -8.1865e+00,\n",
       "          1.0840e+01, -1.4238e+00, -1.4111e+01, -8.7107e+00,  1.0806e+01,\n",
       "          1.3464e+01, -1.1053e+01],\n",
       "        [ 3.5307e+00, -6.1167e+00, -7.7523e+00,  1.1603e+01,  1.2361e+01,\n",
       "          2.1246e+01, -5.0853e+01, -2.4711e+00, -1.3251e+01, -1.5714e+01,\n",
       "          5.7926e+01, -2.3510e+01,  2.2106e+01, -2.1171e+01,  1.6980e+01,\n",
       "          2.9062e+01,  1.9111e+01,  2.9771e+01, -5.1988e+00,  1.3259e+01,\n",
       "         -2.8121e+01, -2.1592e+01,  1.2597e+01, -1.6221e+01, -1.6764e+01,\n",
       "         -1.5168e+01, -2.8787e+01],\n",
       "        [ 3.6369e+00,  3.0836e+00, -1.7137e+01,  2.5571e+01,  2.1973e+01,\n",
       "          2.0393e+01, -1.5527e+01,  2.8162e+00,  5.8919e-01, -1.5409e+01,\n",
       "          2.3826e+01, -1.6159e+01,  2.1628e+00, -7.6228e+00,  1.6092e+01,\n",
       "          1.4674e+01,  1.5310e+01, -2.0917e+00, -2.0696e+00,  4.1002e+01,\n",
       "         -4.6456e+01, -4.6627e+01,  2.8747e+01,  1.3542e+01, -1.4019e+01,\n",
       "         -8.0630e+00, -2.9504e+01],\n",
       "        [ 2.1104e+01,  2.2786e+01,  4.7052e+00,  1.6912e+01, -1.7400e+00,\n",
       "         -8.7520e+00,  6.1856e+00,  2.2162e+01,  4.5414e+00, -2.8936e+01,\n",
       "         -1.3363e+01, -2.7890e+01, -1.6742e+00, -2.9637e+01,  1.0605e+01,\n",
       "          8.0572e+00,  3.5485e+00, -7.0656e+00,  1.3738e+00,  4.1417e+01,\n",
       "         -2.7800e+01, -5.5326e+01,  2.2802e+01, -4.0373e+00, -1.2173e+01,\n",
       "          1.1747e+01, -2.8747e+01],\n",
       "        [-5.0118e+00, -1.8350e+01,  4.4194e+00, -5.9754e+00, -7.7581e+00,\n",
       "         -2.7928e+00,  5.2863e+00,  4.6215e+00,  5.9882e+00,  7.7486e+00,\n",
       "         -1.0862e+01,  3.4831e+00,  1.4435e+00,  1.1836e+01, -3.3570e+00,\n",
       "          7.0196e+00, -1.9790e+01, -7.7477e+00,  7.5606e+00, -5.7518e+00,\n",
       "          7.2342e+00, -3.9326e+00, -5.8033e+00, -1.1352e+00,  2.0169e+00,\n",
       "          2.5451e+00, -1.1941e+01],\n",
       "        [ 1.4893e+01, -3.2518e+00, -1.2037e+01,  5.2242e-01, -5.0713e+00,\n",
       "         -9.9484e+00, -2.1563e+00,  1.6010e+01, -1.0530e+01, -9.3250e+00,\n",
       "         -1.7717e+01,  1.3065e+01, -2.3917e+00, -1.0806e+01, -4.2118e+00,\n",
       "         -2.5887e+01,  8.7071e+00, -7.0675e+00,  9.3381e-01,  2.1193e+01,\n",
       "          6.5239e+00, -1.0392e+01,  9.6785e+00,  1.9323e+01, -1.1931e+01,\n",
       "          1.7028e+00, -1.5511e+01],\n",
       "        [-9.9500e+00, -2.3347e+01,  2.5480e+01, -1.9020e+01, -2.1698e+01,\n",
       "         -8.1725e+00, -9.5641e-02,  1.9587e+00,  9.6681e+00,  1.2493e+01,\n",
       "         -3.6076e+00, -9.6932e+00,  1.1473e+01,  9.9466e+00, -4.1801e+00,\n",
       "          2.9507e+01, -3.6906e+01,  5.8377e+00,  1.0606e+01, -3.3666e+01,\n",
       "          2.2493e+01,  9.3216e+00, -2.4267e+01, -3.2598e+01,  9.3975e+00,\n",
       "          6.5453e+00, -8.2201e+00],\n",
       "        [ 1.1623e+01, -2.0408e+01, -3.7814e+01, -4.5998e-01,  7.2150e+00,\n",
       "          5.9548e+00, -3.5524e+01,  5.6235e+00, -3.1305e+01, -2.3816e+00,\n",
       "          1.2058e+01,  3.6708e+01,  4.4628e+00, -6.1871e+00, -5.5508e+00,\n",
       "         -4.5977e+01,  3.4903e+01,  7.5187e+00, -5.8513e+00,  1.9611e+01,\n",
       "          1.1382e+01,  1.6059e+01,  1.1546e+01,  4.0030e+01, -2.0717e+01,\n",
       "         -1.6892e+01, -1.3172e+01],\n",
       "        [ 1.5297e+01,  4.3269e+00,  1.6088e+01,  4.8168e+00, -3.7390e+01,\n",
       "         -3.8972e+00,  1.3505e+00,  3.7487e+01,  2.1324e+01, -4.8954e+00,\n",
       "         -7.2562e+01, -4.6335e+01, -2.4976e+00, -1.7252e+01,  9.9140e+00,\n",
       "         -3.3249e+00, -6.8574e+00, -5.0592e-01,  1.5020e+01,  4.2359e+01,\n",
       "         -1.0093e+01, -2.6127e+01, -6.9731e+00, -6.5456e+00,  1.7240e+01,\n",
       "          4.6297e+01, -2.9058e+01],\n",
       "        [-9.7985e+00, -5.0825e+01,  5.8697e+00, -1.9909e+01, -1.4619e+01,\n",
       "         -1.8673e+01,  1.1756e+01,  1.3358e+01,  5.1581e+00,  6.9943e+00,\n",
       "         -4.0641e-01,  2.9673e+01,  1.0992e+01,  2.6038e+01, -1.5436e+01,\n",
       "          1.2702e+01, -5.0135e+01, -2.2367e+01,  1.4665e+01, -2.6152e+01,\n",
       "          3.1658e+01, -1.3126e+01, -5.1610e+00, -5.7185e+00, -1.2326e+01,\n",
       "         -1.0093e+01, -3.7592e+01],\n",
       "        [ 3.5307e+00, -6.1167e+00, -7.7523e+00,  1.1603e+01,  1.2361e+01,\n",
       "          2.1246e+01, -5.0853e+01, -2.4711e+00, -1.3251e+01, -1.5714e+01,\n",
       "          5.7926e+01, -2.3510e+01,  2.2106e+01, -2.1171e+01,  1.6980e+01,\n",
       "          2.9062e+01,  1.9111e+01,  2.9771e+01, -5.1988e+00,  1.3259e+01,\n",
       "         -2.8121e+01, -2.1592e+01,  1.2597e+01, -1.6221e+01, -1.6764e+01,\n",
       "         -1.5168e+01, -2.8787e+01],\n",
       "        [ 8.4722e+00,  3.7080e+00, -1.9397e+01,  2.4952e+01,  1.6764e+01,\n",
       "          2.0608e+01, -2.4541e+01,  7.7808e+00, -3.2077e+00, -1.7281e+01,\n",
       "          1.8645e+01, -1.8891e+01,  3.9625e+00, -1.4606e+01,  1.6927e+01,\n",
       "          6.4712e+00,  2.3483e+01,  4.0191e+00, -2.4310e+00,  4.7013e+01,\n",
       "         -4.3903e+01, -4.2346e+01,  2.7441e+01,  1.5059e+01, -1.4090e+01,\n",
       "         -3.9686e+00, -3.1644e+01],\n",
       "        [ 4.7783e+00,  6.1138e+00,  1.9175e+01,  7.2442e+00, -8.1375e-02,\n",
       "         -6.7814e+00,  4.9086e+00,  8.8070e+00,  1.0297e+01, -1.9899e+01,\n",
       "          1.7137e+01, -2.8774e+01,  8.9679e+00, -1.4708e+01,  9.2614e+00,\n",
       "          4.3030e+01, -2.2394e+01, -2.4637e+00,  4.1153e+00,  6.0822e+00,\n",
       "         -2.0750e+01, -4.9798e+01,  1.1448e+01, -3.0146e+01, -1.0170e+01,\n",
       "          3.5268e-01, -2.8980e+01],\n",
       "        [ 1.6514e+01, -9.4101e+00, -3.0296e+01,  2.9795e+00,  1.9503e+00,\n",
       "          7.8227e+00, -4.2368e+01,  9.5668e+00, -2.8650e+01, -8.2013e+00,\n",
       "          9.3964e+00,  1.5887e+01,  7.0672e+00, -1.9129e+01,  1.0378e+00,\n",
       "         -3.7978e+01,  3.9949e+01,  1.7245e+01, -6.1451e+00,  2.7019e+01,\n",
       "          3.0649e+00,  1.0272e+01,  1.0170e+01,  2.8122e+01, -1.7090e+01,\n",
       "         -8.0388e+00, -1.4762e+01],\n",
       "        [ 8.2904e+00, -1.3313e+01,  1.9951e+01, -6.7567e+00, -4.0162e+01,\n",
       "          3.9313e+00, -3.4007e+01,  2.7505e+01,  9.6633e+00, -1.6781e-01,\n",
       "         -3.1168e+01, -4.8131e+01,  1.7145e+01, -1.9820e+01,  1.1205e+01,\n",
       "          1.4589e+01, -5.7294e+00,  2.7120e+01,  1.2622e+01,  1.3982e+01,\n",
       "          2.9338e+00, -2.5623e+00, -2.0608e+01, -2.9979e+01,  1.4205e+01,\n",
       "          3.3663e+01, -3.0872e+01],\n",
       "        [-1.2299e+01, -5.7028e+01, -1.7988e+01, -6.7927e+00,  4.7819e+00,\n",
       "          4.5706e+00, -1.7224e+01,  5.6667e+00, -5.4112e+00,  5.3731e+00,\n",
       "          3.0952e+01,  3.3056e+01,  1.5880e+01,  2.6345e+01, -6.9185e+00,\n",
       "          8.5529e+00, -2.3681e+01, -1.2086e+01,  8.5381e+00, -8.0098e+00,\n",
       "          9.3949e+00, -1.5627e+01,  8.7263e+00,  1.2372e+01, -2.1207e+01,\n",
       "         -2.4349e+01, -4.5823e+01],\n",
       "        [ 1.0129e+01, -1.1615e+00, -1.7061e+00,  8.3853e+00,  1.1788e+01,\n",
       "         -1.6906e+01,  1.7459e+01,  1.2323e+01, -1.1177e+00, -2.5779e+01,\n",
       "          1.7924e+01,  9.3011e+00,  1.9681e+00, -8.3031e+00, -1.2896e+00,\n",
       "          1.4877e+01, -1.8167e+01, -2.3622e+01,  1.4776e+00,  1.4601e+01,\n",
       "         -1.2465e+01, -5.7671e+01,  2.8700e+01,  5.1898e-01, -2.8011e+01,\n",
       "         -1.5136e+01, -3.5396e+01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = hidden_layer @ W2 + b2\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp() # exponentiate the logits to get the fake counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising the logits into a probability\n",
    "probabilities = counts / counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3789e-24, 1.5367e-28, 2.9940e-29, 7.6247e-21, 1.6267e-20, 1.1753e-16,\n",
       "         0.0000e+00, 5.8864e-27, 1.2250e-31, 1.0439e-32, 1.0000e+00, 4.2940e-36,\n",
       "         2.7780e-16, 4.4517e-35, 1.6498e-18, 2.9137e-13, 1.3890e-17, 5.9204e-13,\n",
       "         3.8478e-28, 3.9944e-20, 4.2678e-38, 2.9235e-35, 2.0593e-20, 6.2876e-33,\n",
       "         3.6508e-33, 1.8023e-32, 2.1925e-38],\n",
       "        [1.3059e-26, 1.8402e-26, 8.7868e-28, 3.4880e-17, 3.4754e-14, 9.8025e-16,\n",
       "         3.4195e-35, 1.4604e-28, 1.0313e-25, 4.0462e-30, 1.0000e+00, 7.1677e-32,\n",
       "         2.9542e-19, 6.6298e-27, 4.8765e-18, 3.0290e-08, 8.3257e-23, 9.5610e-21,\n",
       "         3.1301e-26, 3.0615e-19, 2.2289e-41, 9.4349e-41, 6.0642e-16, 1.5977e-28,\n",
       "         1.6172e-31, 3.4521e-33, 9.4161e-36],\n",
       "        [2.7828e-20, 6.0573e-09, 1.3287e-27, 1.9141e-05, 1.2022e-02, 9.4093e-19,\n",
       "         1.3316e-03, 7.2901e-24, 6.1117e-16, 4.8625e-31, 2.1505e-16, 1.0473e-22,\n",
       "         1.8884e-30, 3.5827e-20, 1.0897e-16, 9.5566e-14, 7.3224e-22, 6.6958e-39,\n",
       "         2.3005e-23, 1.3706e-01, 0.0000e+00, 0.0000e+00, 8.4956e-01, 1.4803e-09,\n",
       "         4.6542e-29, 1.2063e-27, 1.9345e-27],\n",
       "        [2.6119e-26, 7.1389e-08, 1.3504e-25, 5.4474e-24, 2.8065e-27, 2.3822e-44,\n",
       "         1.0000e+00, 3.3860e-31, 2.1037e-24, 2.4102e-43, 0.0000e+00, 9.1320e-41,\n",
       "         0.0000e+00, 1.4623e-39, 2.1579e-32, 4.7596e-30, 6.4038e-40, 0.0000e+00,\n",
       "         6.3687e-34, 7.2223e-20, 0.0000e+00, 0.0000e+00, 3.4387e-22, 5.7502e-31,\n",
       "         2.4412e-33, 3.7162e-27, 5.0892e-27],\n",
       "        [5.8009e-18, 6.0488e-13, 9.8731e-15, 9.8772e-28, 3.5202e-28, 3.7800e-27,\n",
       "         9.0622e-14, 4.9800e-23, 6.2230e-23, 7.3549e-14, 3.4497e-39, 1.0329e-18,\n",
       "         2.3186e-26, 7.9994e-23, 4.7313e-25, 2.0551e-31, 2.6930e-19, 4.7149e-18,\n",
       "         3.8698e-21, 4.3401e-29, 4.4738e-07, 9.4032e-01, 2.5679e-32, 6.5035e-21,\n",
       "         3.5098e-12, 1.9131e-13, 5.9677e-02],\n",
       "        [2.3789e-24, 1.5367e-28, 2.9940e-29, 7.6247e-21, 1.6267e-20, 1.1753e-16,\n",
       "         0.0000e+00, 5.8864e-27, 1.2250e-31, 1.0439e-32, 1.0000e+00, 4.2940e-36,\n",
       "         2.7780e-16, 4.4517e-35, 1.6498e-18, 2.9137e-13, 1.3890e-17, 5.9204e-13,\n",
       "         3.8478e-28, 3.9944e-20, 4.2678e-38, 2.9235e-35, 2.0593e-20, 6.2876e-33,\n",
       "         3.6508e-33, 1.8023e-32, 2.1925e-38],\n",
       "        [1.5934e-25, 1.3525e-25, 2.5662e-27, 2.3164e-16, 9.5389e-14, 5.3209e-15,\n",
       "         8.9095e-35, 2.0543e-27, 3.9724e-25, 1.5805e-29, 1.0000e+00, 2.8481e-31,\n",
       "         1.3491e-18, 1.4733e-26, 2.9397e-17, 2.8591e-08, 1.6389e-21, 6.9054e-20,\n",
       "         1.6891e-25, 8.6674e-18, 1.2013e-40, 5.8439e-40, 3.7682e-15, 2.2034e-27,\n",
       "         9.0883e-31, 4.3668e-32, 3.4320e-35],\n",
       "        [3.7498e-18, 4.6414e-18, 9.1080e-40, 7.6478e-18, 2.8250e-25, 4.7538e-27,\n",
       "         8.6936e-38, 1.1613e-21, 6.7340e-35, 6.4460e-44, 7.2904e-33, 4.4655e-39,\n",
       "         3.7481e-32, 0.0000e+00, 1.8481e-23, 7.0577e-39, 3.8656e-12, 2.6214e-29,\n",
       "         4.5232e-33, 1.0000e+00, 0.0000e+00, 0.0000e+00, 3.4189e-15, 1.8130e-19,\n",
       "         4.3946e-38, 1.3070e-27, 2.0193e-41],\n",
       "        [1.4394e-13, 4.5226e-15, 1.1720e-03, 1.3536e-13, 2.2211e-25, 2.0167e-16,\n",
       "         1.1137e-07, 9.5789e-05, 1.0773e-02, 2.5607e-14, 2.8487e-34, 3.5907e-31,\n",
       "         1.0785e-14, 5.7018e-15, 3.3596e-11, 4.1967e-04, 6.7979e-27, 7.2207e-18,\n",
       "         8.4279e-08, 2.5259e-07, 8.3935e-19, 1.7042e-27, 9.8177e-18, 8.8472e-23,\n",
       "         1.1564e-07, 9.8754e-01, 1.1812e-24],\n",
       "        [5.9704e-12, 3.1445e-28, 1.7032e-23, 1.9681e-20, 1.7918e-18, 1.4524e-22,\n",
       "         3.1613e-20, 1.7207e-09, 5.7143e-24, 4.9655e-19, 1.8641e-15, 9.9998e-01,\n",
       "         2.2998e-13, 1.6992e-16, 1.9371e-21, 3.1682e-27, 2.4098e-17, 4.2084e-20,\n",
       "         4.6655e-15, 7.4312e-15, 1.5289e-05, 4.7763e-18, 1.3204e-12, 6.4236e-08,\n",
       "         6.1107e-26, 9.1839e-22, 5.7430e-28],\n",
       "        [5.8868e-08, 5.6729e-16, 2.5726e-02, 4.0562e-11, 4.1734e-15, 4.2482e-10,\n",
       "         1.9752e-04, 2.5346e-01, 8.9843e-03, 1.5687e-03, 1.8725e-17, 6.3826e-08,\n",
       "         1.7448e-05, 6.9793e-03, 1.2890e-08, 4.5248e-04, 7.7855e-18, 7.8112e-10,\n",
       "         5.4324e-02, 4.2652e-09, 5.9570e-01, 1.3651e-08, 1.3821e-11, 8.3840e-10,\n",
       "         1.3775e-04, 5.2448e-02, 2.2088e-14],\n",
       "        [4.7069e-14, 1.1432e-23, 1.0547e-15, 3.3656e-17, 2.5329e-12, 3.4993e-18,\n",
       "         4.1534e-11, 2.1738e-12, 1.5559e-16, 3.7417e-13, 2.6699e-08, 9.9999e-01,\n",
       "         1.4054e-11, 1.4608e-08, 8.0915e-18, 6.8551e-14, 3.9466e-21, 5.3380e-19,\n",
       "         6.4029e-12, 6.3288e-19, 6.9917e-06, 5.2572e-16, 1.5280e-11, 2.0806e-10,\n",
       "         7.7602e-20, 3.1586e-20, 3.8186e-20],\n",
       "        [2.3789e-24, 1.5367e-28, 2.9940e-29, 7.6247e-21, 1.6267e-20, 1.1753e-16,\n",
       "         0.0000e+00, 5.8864e-27, 1.2250e-31, 1.0439e-32, 1.0000e+00, 4.2940e-36,\n",
       "         2.7780e-16, 4.4517e-35, 1.6498e-18, 2.9137e-13, 1.3890e-17, 5.9204e-13,\n",
       "         3.8478e-28, 3.9944e-20, 4.2678e-38, 2.9235e-35, 2.0593e-20, 6.2876e-33,\n",
       "         3.6508e-33, 1.8023e-32, 2.1925e-38],\n",
       "        [1.2169e-16, 9.8883e-16, 9.8678e-24, 1.2636e-05, 6.1482e-06, 4.1823e-08,\n",
       "         3.5223e-21, 4.9550e-17, 1.3125e-15, 4.3859e-23, 3.7542e-05, 3.5649e-23,\n",
       "         1.8499e-16, 2.6310e-18, 3.8413e-10, 2.6835e-08, 1.7560e-12, 1.7427e-19,\n",
       "         1.0574e-17, 9.9948e-01, 8.8621e-38, 2.5142e-38, 4.6644e-04, 1.9918e-11,\n",
       "         5.6964e-23, 1.6141e-21, 3.9196e-29],\n",
       "        [4.4844e-12, 1.4383e-10, 2.0274e-24, 5.2696e-15, 2.7222e-24, 2.1113e-28,\n",
       "         7.5531e-21, 6.8592e-13, 1.7068e-23, 1.7695e-38, 1.1934e-33, 1.3253e-35,\n",
       "         1.1554e-26, 9.5836e-40, 2.7175e-19, 3.8175e-26, 6.7976e-18, 7.6387e-28,\n",
       "         2.4592e-24, 1.0000e+00, 2.0705e-37, 0.0000e+00, 6.2249e-12, 1.2387e-21,\n",
       "         4.3785e-30, 8.3957e-18, 1.0802e-35],\n",
       "        [4.4775e-10, 2.0986e-15, 2.5630e-01, 2.3494e-11, 3.1203e-15, 1.6854e-09,\n",
       "         9.2216e-03, 1.5714e-03, 2.6930e-01, 4.2473e-02, 2.4237e-19, 4.1753e-10,\n",
       "         4.5724e-07, 1.7394e-01, 1.0671e-08, 1.6410e-02, 1.2154e-19, 1.0218e-10,\n",
       "         3.0970e-02, 6.9172e-11, 1.2678e-02, 5.9828e-08, 1.8491e-13, 4.0952e-11,\n",
       "         1.2256e-02, 1.7489e-01, 3.9350e-12],\n",
       "        [2.3789e-24, 1.5367e-28, 2.9940e-29, 7.6247e-21, 1.6267e-20, 1.1753e-16,\n",
       "         0.0000e+00, 5.8864e-27, 1.2250e-31, 1.0439e-32, 1.0000e+00, 4.2940e-36,\n",
       "         2.7780e-16, 4.4517e-35, 1.6498e-18, 2.9137e-13, 1.3890e-17, 5.9204e-13,\n",
       "         3.8478e-28, 3.9944e-20, 4.2678e-38, 2.9235e-35, 2.0593e-20, 6.2876e-33,\n",
       "         3.6508e-33, 1.8023e-32, 2.1925e-38],\n",
       "        [5.9231e-17, 3.4061e-17, 5.6280e-26, 1.9882e-07, 5.4428e-09, 1.1211e-09,\n",
       "         2.8178e-25, 2.6071e-17, 2.8116e-18, 3.1699e-25, 3.4731e-08, 1.4966e-25,\n",
       "         1.3564e-17, 7.6300e-22, 1.5195e-11, 3.6821e-12, 6.9556e-12, 1.9260e-19,\n",
       "         1.9691e-19, 1.0000e+00, 1.0412e-38, 8.7741e-39, 4.7625e-06, 1.1864e-12,\n",
       "         1.2731e-24, 4.9133e-22, 2.3970e-31],\n",
       "        [1.5080e-09, 8.1062e-09, 1.1387e-16, 2.2799e-11, 1.8084e-19, 1.6294e-22,\n",
       "         5.0043e-16, 4.3454e-09, 9.6670e-17, 2.7954e-31, 1.6205e-24, 7.9564e-31,\n",
       "         1.9314e-19, 1.3867e-31, 4.1550e-14, 3.2523e-15, 3.5815e-17, 8.7988e-22,\n",
       "         4.0703e-18, 1.0000e+00, 8.7022e-31, 9.6690e-43, 8.2392e-09, 1.8180e-20,\n",
       "         5.3224e-24, 1.3018e-13, 3.3759e-31],\n",
       "        [4.5713e-08, 7.3705e-14, 5.7010e-04, 1.7440e-08, 2.9329e-09, 4.2045e-07,\n",
       "         1.3565e-03, 6.9780e-04, 2.7369e-03, 1.5915e-02, 1.3163e-10, 2.2352e-04,\n",
       "         2.9075e-05, 9.4795e-01, 2.3915e-07, 7.6767e-03, 1.7459e-14, 2.9638e-09,\n",
       "         1.3187e-02, 2.1809e-08, 9.5150e-03, 1.3449e-07, 2.0716e-08, 2.2061e-06,\n",
       "         5.1590e-05, 8.7487e-05, 4.4733e-11],\n",
       "        [1.5814e-03, 2.0830e-11, 3.1856e-15, 9.0747e-10, 3.3770e-12, 2.5728e-14,\n",
       "         6.2300e-11, 4.8294e-03, 1.4378e-14, 4.7989e-14, 1.0876e-17, 2.5404e-04,\n",
       "         4.9230e-11, 1.0908e-14, 7.9763e-12, 3.0773e-21, 3.2540e-06, 4.5875e-13,\n",
       "         1.3693e-09, 8.6061e-01, 3.6666e-07, 1.6504e-14, 8.5957e-06, 1.3271e-01,\n",
       "         3.5434e-15, 2.9544e-09, 9.8721e-17],\n",
       "        [7.1788e-18, 1.0911e-23, 1.7508e-02, 8.2584e-22, 5.6763e-23, 4.2464e-17,\n",
       "         1.3670e-13, 1.0664e-12, 2.3774e-09, 4.0091e-08, 4.0786e-15, 9.2811e-18,\n",
       "         1.4454e-08, 3.1408e-09, 2.3009e-15, 9.8161e-01, 1.4101e-29, 5.1591e-11,\n",
       "         6.0740e-09, 3.6003e-28, 8.8257e-04, 1.6812e-09, 4.3473e-24, 1.0476e-27,\n",
       "         1.8137e-09, 1.0469e-10, 4.0489e-17],\n",
       "        [4.4170e-13, 5.4244e-27, 1.4961e-34, 2.4979e-18, 5.3800e-15, 1.5257e-15,\n",
       "         1.4779e-33, 1.0954e-15, 1.0037e-31, 3.6561e-19, 6.8235e-13, 3.4637e-02,\n",
       "         3.4318e-16, 8.1346e-21, 1.5370e-20, 4.2621e-38, 5.6952e-03, 7.2889e-15,\n",
       "         1.1380e-20, 1.3010e-09, 3.4714e-13, 3.7281e-11, 4.0882e-13, 9.5967e-01,\n",
       "         3.9803e-27, 1.8251e-25, 7.5276e-24],\n",
       "        [3.3780e-14, 5.8110e-19, 7.4451e-14, 9.4850e-19, 4.4355e-37, 1.5580e-22,\n",
       "         2.9624e-20, 1.4641e-04, 1.3994e-11, 5.7424e-23, 0.0000e+00, 5.7825e-41,\n",
       "         6.3157e-22, 2.4701e-28, 1.5514e-16, 2.7614e-22, 8.0723e-24, 4.6281e-21,\n",
       "         2.5604e-14, 1.9109e-02, 3.1750e-25, 3.4527e-32, 7.1906e-24, 1.1026e-23,\n",
       "         2.3576e-13, 9.8075e-01, 1.8431e-33],\n",
       "        [8.6788e-19, 1.3213e-36, 5.5344e-12, 3.5276e-23, 6.9994e-21, 1.2138e-22,\n",
       "         1.9935e-09, 9.8905e-09, 2.7167e-12, 1.7040e-11, 1.0408e-14, 1.2040e-01,\n",
       "         9.2810e-10, 3.1770e-03, 3.0926e-21, 5.1295e-09, 2.6344e-36, 3.0202e-24,\n",
       "         3.6555e-08, 6.8559e-26, 8.7642e-01, 3.1142e-20, 8.9642e-17, 5.1332e-17,\n",
       "         6.9295e-20, 6.4649e-19, 7.3769e-31],\n",
       "        [2.3789e-24, 1.5367e-28, 2.9940e-29, 7.6247e-21, 1.6267e-20, 1.1753e-16,\n",
       "         0.0000e+00, 5.8864e-27, 1.2250e-31, 1.0439e-32, 1.0000e+00, 4.2940e-36,\n",
       "         2.7780e-16, 4.4517e-35, 1.6498e-18, 2.9137e-13, 1.3890e-17, 5.9204e-13,\n",
       "         3.8478e-28, 3.9944e-20, 4.2678e-38, 2.9235e-35, 2.0593e-20, 6.2876e-33,\n",
       "         3.6508e-33, 1.8023e-32, 2.1925e-38],\n",
       "        [1.8288e-17, 1.5599e-19, 1.4416e-29, 2.6248e-10, 7.2979e-14, 3.4098e-12,\n",
       "         8.4084e-32, 9.1598e-18, 1.5476e-22, 1.1954e-28, 4.7865e-13, 2.3915e-29,\n",
       "         2.0120e-19, 1.7350e-27, 8.5895e-14, 2.4725e-18, 6.0418e-11, 2.1292e-19,\n",
       "         3.3648e-22, 1.0000e+00, 3.2793e-40, 1.5558e-39, 3.1633e-09, 1.3267e-14,\n",
       "         2.9083e-27, 7.2311e-23, 6.9203e-35],\n",
       "        [2.4399e-17, 9.2757e-17, 4.3613e-11, 2.8725e-16, 1.8916e-19, 2.3283e-22,\n",
       "         2.7792e-17, 1.3709e-15, 6.0805e-15, 4.6784e-28, 5.6852e-12, 6.5416e-32,\n",
       "         1.6102e-15, 8.4085e-26, 2.1595e-15, 1.0000e+00, 3.8582e-29, 1.7465e-20,\n",
       "         1.2572e-17, 8.9874e-17, 1.9983e-28, 4.8422e-41, 1.9227e-14, 1.6597e-32,\n",
       "         7.8566e-24, 2.9196e-19, 5.3268e-32],\n",
       "        [6.6386e-11, 3.6605e-22, 3.1116e-31, 8.7950e-17, 3.1424e-17, 1.1159e-14,\n",
       "         1.7780e-36, 6.3839e-14, 1.6133e-30, 1.2260e-21, 5.3839e-14, 3.5464e-11,\n",
       "         5.2421e-15, 2.2007e-26, 1.2617e-17, 1.4339e-34, 9.9999e-01, 1.3792e-10,\n",
       "         9.5832e-21, 2.4233e-06, 9.5799e-17, 1.2920e-13, 1.1667e-13, 7.3065e-06,\n",
       "         1.6908e-25, 1.4424e-21, 1.7354e-24],\n",
       "        [9.5343e-12, 3.9525e-21, 1.1057e-06, 2.7824e-18, 8.6467e-33, 1.2194e-13,\n",
       "         4.0696e-30, 2.1084e-03, 3.7629e-11, 2.0227e-15, 6.9650e-29, 2.9897e-36,\n",
       "         6.6805e-08, 5.9050e-24, 1.7590e-10, 5.1827e-09, 7.7723e-18, 1.4356e-03,\n",
       "         7.2494e-10, 2.8249e-09, 4.4973e-14, 1.8451e-16, 2.6852e-24, 2.2867e-28,\n",
       "         3.5327e-09, 9.9645e-01, 9.3625e-29],\n",
       "        [1.7876e-20, 6.7071e-40, 6.0461e-23, 4.4007e-18, 4.6807e-13, 3.7893e-13,\n",
       "         1.2981e-22, 1.1338e-12, 1.7519e-17, 8.4535e-13, 1.0862e-01, 8.9029e-01,\n",
       "         3.0905e-08, 1.0837e-03, 3.8806e-18, 2.0326e-11, 2.0363e-25, 2.2103e-20,\n",
       "         2.0027e-11, 1.3030e-18, 4.7173e-11, 6.4089e-22, 2.4172e-11, 9.2644e-10,\n",
       "         2.4184e-24, 1.0446e-25, 4.9314e-35],\n",
       "        [8.6035e-09, 1.0749e-13, 6.2349e-14, 1.5048e-09, 4.5226e-08, 1.5620e-20,\n",
       "         1.3131e-05, 7.7232e-08, 1.1230e-13, 2.1889e-24, 2.0903e-05, 3.7603e-09,\n",
       "         2.4576e-12, 8.5076e-17, 9.4562e-14, 9.9312e-07, 4.4238e-21, 1.8912e-23,\n",
       "         1.5049e-12, 7.5298e-07, 1.3247e-18, 3.0886e-38, 9.9996e-01, 5.7701e-13,\n",
       "         2.3484e-25, 9.1687e-20, 1.4575e-28]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0].sum() # every row sum upto 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y has the identity of the next character in the sequence that we would love to predict. \n",
    "\n",
    "We would like to index into the rows of probabilties in each row and we would like to pluck out the probabilty assigned to the correct character as given here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1753e-16, 6.6298e-27, 3.5827e-20, 7.1389e-08, 5.8009e-18, 2.9137e-13,\n",
       "        1.3491e-18, 6.4460e-44, 9.8177e-18, 4.9655e-19, 5.6729e-16, 4.7069e-14,\n",
       "        1.5367e-28, 4.6644e-04, 1.4383e-10, 4.4775e-10, 1.0439e-32, 1.0000e+00,\n",
       "        8.1062e-09, 5.7010e-04, 2.5728e-14, 1.4454e-08, 3.4318e-16, 5.8110e-19,\n",
       "        8.6788e-19, 3.9944e-20, 2.4725e-18, 3.8582e-29, 1.6133e-30, 2.0227e-15,\n",
       "        6.7071e-40, 8.6035e-09])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[torch.arange(32),Y] # torch.arange(32) iterates the rows and grabs coulmns provided by Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values generated are some what not ok. From the values of some of the probabilities, the network thinks that some are extremely unlikely to happen. Also these values should be 1 at the end of training then only we can say we r correctly predicting the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1753e-16, -6.6298e-27, -3.5827e-20, -7.1389e-08, -5.8009e-18,\n",
       "        -2.9137e-13, -1.3491e-18, -6.4460e-44, -9.8177e-18, -4.9655e-19,\n",
       "        -5.6729e-16, -4.7069e-14, -1.5367e-28, -4.6644e-04, -1.4383e-10,\n",
       "        -4.4775e-10, -1.0439e-32, -1.0000e+00, -8.1062e-09, -5.7010e-04,\n",
       "        -2.5728e-14, -1.4454e-08, -3.4318e-16, -5.8110e-19, -8.6788e-19,\n",
       "        -3.9944e-20, -2.4725e-18, -3.8582e-29, -1.6133e-30, -2.0227e-15,\n",
       "        -6.7071e-40, -8.6035e-09])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probabilities[torch.arange(32),Y]\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss needs to be minimsed so that the network can correctly predict the correct character in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducability\n",
    "C = torch.randn((27,2), generator=g)\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100,27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to have a peak into the number of parameters in total\n",
    "parameters = [C, W1, b1, W2, b2] # clustering of parameters\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = C[X]\n",
    "h = torch.tanh(embedding.view(-1,6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum(1, keepdim=True)\n",
    "loss = - probabilities[torch.arange(32), Y]\n",
    "loss.log().mean() # how well the current networks work based on the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we loook at it one perspective, when we calculate loss based on logits, we are just doing classification and to calculate this simply we can use the cross entropy function in pytorch\n",
    "\n",
    "```python\n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum(1, keepdim=True)\n",
    "loss = - probabilities[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "could be reduced to \n",
    "\n",
    "```python\n",
    "F.cross_entropy(logits, Y)\n",
    "```\n",
    "\n",
    "<u>Advantages of using cross entropy </u>\n",
    "\n",
    "- Unnecessary creation of a lot of tensors.<br>\n",
    "    It makes the whole process fairly inefficient to run like this. By this single line of code we skip the creation of all these intermediate tensors and cluster up all these operations and very often have fused kernels that very efficiently evaluate these expressions.\n",
    "\n",
    "- Backward pass can be made much more efficient<br>\n",
    "    Not only because its a fused kernel but also that the expressions since they are clustered are of much simpler form mathematically. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([-2,-3,0,5])\n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy is also mathematically well behaved<br>\n",
    "    Given [-2,-3,0,5] these values the probability distribution is well and good. But when these take up some extreme values which can happen because of the optimisation of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 3.3311e-04, 6.6906e-03, 9.9298e-01])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example\n",
    "logits = torch.tensor([-100,-3,0,5])\n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 4.9787e-02, 1.0000e+00, 1.4841e+02])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having these negative numbers is also not much of a headache. it will cause a very very small number thats close to 0 something like 3.7835e-44."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example\n",
    "logits = torch.tensor([-2,-3,0,100])\n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having really big positive values, we could run into trouble even ending up with a ```nan```. The reason for these is that counts do have a value ```infinite```. We will run out of the range of floating point numbers \n",
    "(e^{100}) run out of the dynamic range of floating point number. So passing in very large logits is not possible in these expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1353, 0.0498, 1.0000,    inf])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These operations allow us to produce the same output even after adding offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example\n",
    "logits = torch.tensor([-2,-3,0,5]) \n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example\n",
    "logits = torch.tensor([-2,-3,0,5]) + 7\n",
    "counts = logits.exp()\n",
    "probabilities = counts / counts.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we normalise the logits, we get the same output for the logits even after adding in some offsets to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7380964756011963\n",
      "1.6535115242004395\n",
      "1.579089879989624\n",
      "1.5117665529251099\n",
      "1.4496047496795654\n",
      "1.3913121223449707\n",
      "1.3359923362731934\n",
      "1.2830528020858765\n",
      "1.2321909666061401\n",
      "1.1833815574645996\n",
      "1.1367987394332886\n",
      "1.0926642417907715\n",
      "1.0510926246643066\n",
      "1.0120267868041992\n",
      "0.9752707481384277\n",
      "0.9405564069747925\n",
      "0.9076125025749207\n",
      "0.876192033290863\n",
      "0.8460890650749207\n",
      "0.8171358108520508\n",
      "0.7891990542411804\n",
      "0.762174665927887\n",
      "0.7359814047813416\n",
      "0.7105578184127808\n",
      "0.6858609914779663\n",
      "0.6618651151657104\n",
      "0.638565719127655\n",
      "0.6159818768501282\n",
      "0.5941659212112427\n",
      "0.573210597038269\n",
      "0.5532563924789429\n",
      "0.5344881415367126\n",
      "0.5171169638633728\n",
      "0.5013313293457031\n",
      "0.48724284768104553\n",
      "0.47484058141708374\n",
      "0.46399781107902527\n",
      "0.4545145034790039\n",
      "0.44617095589637756\n",
      "0.4387665092945099\n",
      "0.432133287191391\n",
      "0.42613884806632996\n",
      "0.4206799268722534\n",
      "0.41567549109458923\n",
      "0.4110615849494934\n",
      "0.40678712725639343\n",
      "0.4028107225894928\n",
      "0.39909741282463074\n",
      "0.3956180810928345\n",
      "0.3923478424549103\n",
      "0.3892652690410614\n",
      "0.38635197281837463\n",
      "0.38359174132347107\n",
      "0.3809700310230255\n",
      "0.3784741759300232\n",
      "0.37609297037124634\n",
      "0.37381651997566223\n",
      "0.37163490056991577\n",
      "0.36954089999198914\n",
      "0.3675267696380615\n",
      "0.3655855357646942\n",
      "0.3637113571166992\n",
      "0.3618983328342438\n",
      "0.3601416349411011\n",
      "0.35843637585639954\n",
      "0.35677799582481384\n",
      "0.35516276955604553\n",
      "0.35358697175979614\n",
      "0.352046936750412\n",
      "0.3505397140979767\n",
      "0.3490622341632843\n",
      "0.3476121425628662\n",
      "0.34618663787841797\n",
      "0.34478363394737244\n",
      "0.3434009552001953\n",
      "0.34203672409057617\n",
      "0.34068983793258667\n",
      "0.3393586277961731\n",
      "0.3380417823791504\n",
      "0.3367387056350708\n",
      "0.33544862270355225\n",
      "0.3341710865497589\n",
      "0.3329058885574341\n",
      "0.3316529095172882\n",
      "0.33041247725486755\n",
      "0.32918480038642883\n",
      "0.32797059416770935\n",
      "0.32677051424980164\n",
      "0.325585275888443\n",
      "0.3244159519672394\n",
      "0.3232629895210266\n",
      "0.3221277594566345\n",
      "0.32101112604141235\n",
      "0.3199138641357422\n",
      "0.31883662939071655\n",
      "0.31778034567832947\n",
      "0.3167456090450287\n",
      "0.31573280692100525\n",
      "0.3147423267364502\n",
      "0.31377431750297546\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    embedding = C[X]\n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    counts = logits.exp()\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we run it further longer, the loss goes on to reduce further. This works because we are only over fitting 32 examples with 3481 parameters. It is calling overfitting because we are running all these examples over just 32 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31377431750297546\n"
     ]
    }
   ],
   "source": [
    "print(loss.item()) # after training it again and again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be surprising as to not see a 0 after all this training this is because:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([11.4639, 13.4778, 19.0661, 17.9120, 13.2064, 11.4639, 13.2552, 11.8626,\n",
       "        13.6934, 15.6432, 12.8634, 17.9044, 11.4639, 13.2158, 14.3344, 17.2696,\n",
       "        11.4639, 14.0626, 11.7470, 13.5321, 15.9663, 12.5515,  8.1474,  8.1505,\n",
       "        14.0189, 11.4639, 13.5286, 13.8694, 11.3024, 14.4007, 15.8964, 12.3963],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 1, 13, 13,  1,  0,  1, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  1, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0,  1, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1) # outputs the original value and their indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On keeping a closer look, we see the indices are almost equivalent to Y but there are differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore loss is not 0 <br>\n",
    "since in accordance with our example \"...\" predicts e, o, a , i , s as possible outcomes for the subsequent character and therefore not able to completely overfit. But this overfitting correctly works for unique outputs generated by unique inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
