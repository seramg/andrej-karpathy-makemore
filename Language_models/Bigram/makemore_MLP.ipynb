{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier way of using the context of the previously happening single character to predict the next one was good but we weren't producing very name like sounding things.\n",
    "\n",
    "Problem with this approach is\n",
    "The table we saw in the previous file (tells deals with predicting the next character in a sequence in context to the previous one) quickly blows up and its size grows exponentially with the length of context.\n",
    "\n",
    "This is because if we take a single character at a time, we will have 27 different possibilities of context that is 27 different rows. But if we were to consider the previous context of 2 characters to predict the next then we will have 27*27= 729 rows/possibilities of context. and it goes on\n",
    "\n",
    "We will have too many rows and way too many counts for each possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the research paper [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf),\n",
    "\n",
    "it deals with the word level language modelling and it does have a vocabulary of 17000 possible words and each word is assosciated as a 30 dimensional feature vector. Literaly 17000 points or vectors in a 30 dimensional vector space.\n",
    "\n",
    "Initially these words are initialised randomly, spreaded out randomly. Then we tune the embeddings of these words using **backpropogation**. \n",
    "\n",
    "So during the course of training of these neural networks go around in the space and words with almost similar meanings or are synonyms of each other end up in a similar part of space and conversely words with different meanings go away from each other.\n",
    "\n",
    "\n",
    "Similar to our previous work, in the research paper they,\n",
    "- try to predict the next word given the previous ones\n",
    "- to train the neural network, they maximize the log likelihood of the training data\n",
    "So the modelling approach is all identical\n",
    "\n",
    "In the paper a example is sited,\n",
    "\n",
    "The phrase **A dog was running in a room**\n",
    "is likewise to **The cat is running in a room**\n",
    "\n",
    "Mayb the network may have realised that a and the are frequently interchangeable and therefore the network may have put the embeddings of 'a' and 'the' near in the space and therefore the similarity is drawn. Therefore we can transfer knowledge through that embedding and we can generalise in that way. In the same way they may have found similarity in the 'dog' and 'cat' too therefore a mapping happened there too causing a knowledge transfer and a generalisation to novel scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/neural_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we take 3 previous words to predict the upcoming 4th word in a sequence. These 3 are the index of the incoming word and because there are 17k words these indices are integers between 0 and 16999. \n",
    "\n",
    "Along with this we have a lookup table C which is a matrix 17k x 30.\n",
    "\n",
    "This is considered as a lookup table and every index is actually plucking a row out of this embedding matrix so that each index is converted to a 30 dimensional vector that corresponds to the embedding vector for that word.\n",
    "\n",
    "We have the input layer of 30 neurons for 3 words making up 90 neurons and C is being shared across all the words so we are indexing to the same matrix again and again for each one of these words.\n",
    "\n",
    "Size of the hidden layer of this neural network is a _hyper parameter_ and this can be as large or small it can be. We will be working out multiple choices of sizes/different lengths of these hidden layers and then evaluate how well they do.\n",
    "\n",
    "Lets say we have 100 neurons out of which all of them will be fully connected to the 90 words or 90 numbers that make up these three words so this is a fully connected layer and then there is a tan h long linearity.\n",
    "\n",
    "In case of the output layer, there are 17k possible words that could appear next and all of these are fully connected to tall the neurons in the hidden layer.  Since there are a lot of words, there are lot of parameters here and therefore a lot of computation here.\n",
    "\n",
    "Therefore we can also say there are ```17k logits``` here and at the top we have the softmax layer which exponentiates these logits, normalised to sum of 1 to get the best probability distribution for next word in sequence. In training we do have the label/ identity of the next word in the sequence, that index is used to pluck out the probability of that word and they are maximised  the probability of that word with respect to the parameters of the neural network. \n",
    "\n",
    "These parameters are nothing but the ```weights``` and ```biases``` of the output layer and of the hidden kayer and the embedding lookup table of C (matrix). All of these are optimised using the back propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
